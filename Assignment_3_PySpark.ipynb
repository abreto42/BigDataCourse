{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_PySpark.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "2yMqk7MVoyfh",
        "iE7RT7LipZQ7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msaadsadiq/BigDataCourse/blob/master/Assignment_3_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7tV6wYMl7ype",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ECE 795 - Big Data\n",
        "## Assignment #3 - Twitter Sentiment Analysis and Word Count using PySpark Distributed Computing on DataProc "
      ]
    },
    {
      "metadata": {
        "id": "P2dz95IU8QJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Provide your credentials to the runtime"
      ]
    },
    {
      "metadata": {
        "id": "pZrs8Onl5Vns",
        "colab_type": "code",
        "outputId": "8d23621f-6056-45ac-e0d7-460ae8e5c201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Authenticate your student profile\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "48KU9aLS8Yd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set the Project ID and Enable APIs"
      ]
    },
    {
      "metadata": {
        "id": "sUg7m-aj8Rr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_id = 'ece795'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4v97QS235RN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### In GCP, there are many different services; Compute Engine, Cloud Storage, BigQuery, Cloud SQL, Cloud Dataproc to name a few. In order to use any of these services in your project, you first have to enable them."
      ]
    },
    {
      "metadata": {
        "id": "-XAex6YX3_Es",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*rYZZH8w9iScxIXG27qG-ww.png)"
      ]
    },
    {
      "metadata": {
        "id": "MJnvIcKk4Ldc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Put your mouse over “APIs & Services” on the left-side menu, then click into “Library”. For this project, we will enable three APIs: Cloud Dataproc, Compute Engine, and Cloud Storage."
      ]
    },
    {
      "metadata": {
        "id": "vz06iAlA4Grc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*qH5u_JSH2JLZW_SQTcetSQ.png)"
      ]
    },
    {
      "metadata": {
        "id": "fev7JQYYoje6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running Example 1: Word Count"
      ]
    },
    {
      "metadata": {
        "id": "N7VZj2MqqcmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### This word count example will use the Shakespeare dataset in BigQuery. The only difference is that instead of using Hadoop, it uses PySpark which is a Python library for Spark"
      ]
    },
    {
      "metadata": {
        "id": "V4E_-MsuqkvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 1: create a dataset named \"wordcount_dataset\"\n",
        "- Select your project or create a new one, remember to enable billing\n",
        "- Go to Big Query\n",
        "- Create a dataset, and name it wordcount_dataset"
      ]
    },
    {
      "metadata": {
        "id": "2CZUAdrPyx_h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assign3_img11.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "rg4RWPWArcrl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 2: create a cluster in Dataproc and Google Cloud Storage. Go to Dataproc and create a cluster similar to our previous tutorial. It should look like this"
      ]
    },
    {
      "metadata": {
        "id": "0T3sfBOxrfU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img2.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "PiZr43X2rxeS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now click on the link in the “Cloud Storage staging bucket” column, it will bring you to your Cloud\n",
        "Storage. This is where we will upload our python code, which will then give us a link to submit a job to the cluster."
      ]
    },
    {
      "metadata": {
        "id": "igwEA0oOrzmc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 3: modify the code and upload it to Cloud Storage. Download the following code from here\n",
        "\n",
        "Change the input and output directory strings. Replace the {} symbols with your Cloud Storage Bucket id.\n",
        "\n",
        "input_directory =\n",
        "'gs://{}/hadoop/tmp/bigquery/pyspark_input'.\n",
        "output_directory =\n",
        "\n",
        "'gs://{}/hadoop/tmp/bigquery/pyspark_output'\n",
        "\n",
        "Upload the python file to your Cloud Storage. We will use this link as input to our PySpark job."
      ]
    },
    {
      "metadata": {
        "id": "I8eQM0BGoqmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/python\n",
        "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "import json\n",
        "import pprint\n",
        "import subprocess\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "\n",
        "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
        "# by the InputFormat. This assumes the Cloud Storage connector for\n",
        "# Hadoop is configured.\n",
        "bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
        "project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
        "input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
        "\n",
        "conf = {\n",
        "    # Input Parameters.\n",
        "    'mapred.bq.project.id': project,\n",
        "    'mapred.bq.gcs.bucket': bucket,\n",
        "    'mapred.bq.temp.gcs.path': input_directory,\n",
        "    'mapred.bq.input.project.id': 'publicdata',\n",
        "    'mapred.bq.input.dataset.id': 'samples',\n",
        "    'mapred.bq.input.table.id': 'shakespeare',\n",
        "}\n",
        "\n",
        "# Output Parameters.\n",
        "output_dataset = 'wordcount_dataset'\n",
        "output_table = 'wordcount_output'\n",
        "\n",
        "# Load data in from BigQuery.\n",
        "table_data = sc.newAPIHadoopRDD(\n",
        "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
        "    'org.apache.hadoop.io.LongWritable',\n",
        "    'com.google.gson.JsonObject',\n",
        "    conf=conf)\n",
        "\n",
        "# Perform word count.\n",
        "word_counts = (\n",
        "    table_data\n",
        "    .map(lambda record: json.loads(record[1]))\n",
        "    .map(lambda x: (x['word'].lower(), int(x['word_count'])))\n",
        "    .reduceByKey(lambda x, y: x + y))\n",
        "\n",
        "# Display 10 results.\n",
        "pprint.pprint(word_counts.take(10))\n",
        "\n",
        "# Stage data formatted as newline-delimited JSON in Cloud Storage.\n",
        "output_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output'.format(bucket)\n",
        "output_files = output_directory + '/part-*'\n",
        "\n",
        "sql_context = SQLContext(sc)\n",
        "(word_counts\n",
        " .toDF(['word', 'word_count'])\n",
        " .write.format('json').save(output_directory))\n",
        "\n",
        "# Shell out to bq CLI to perform BigQuery import.\n",
        "subprocess.check_call(\n",
        "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
        "    '--replace '\n",
        "    '--autodetect '\n",
        "    '{dataset}.{table} {files}'.format(\n",
        "        dataset=output_dataset, table=output_table, files=output_files\n",
        "    ).split())\n",
        "\n",
        "# Manually clean up the staging_directories, otherwise BigQuery\n",
        "# files will remain indefinitely.\n",
        "input_path = sc._jvm.org.apache.hadoop.fs.Path(input_directory)\n",
        "input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(input_path, True)\n",
        "output_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\n",
        "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
        "    output_path, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_tMGQjoshl1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img3.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "gj7HDcclt5dw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, here I uploaded sparkwc.py, and my link would be gs://dataproc-e4225d08-23a8-481faff9-5205024119a5-us/sparkwc.py"
      ]
    },
    {
      "metadata": {
        "id": "o43Pbiatt7wX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 4: submit the job. Similar to our Hadoop example. Use the settings below. Remember to use your own gs:// link, not mine. "
      ]
    },
    {
      "metadata": {
        "id": "mbmw6HxQuAFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img12.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "fgCZ0uJtyRlq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Step 5: browse the result. Once the job is done (should be around 2~10 minutes). Go back to Big Query and explore the result."
      ]
    },
    {
      "metadata": {
        "id": "gt31KropyUq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img5.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "hzXd1cXAynDK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " #### Reference. \n",
        "https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example\n",
        "https://piazza.com/class_profile/get_resource/j1318wb5md21wb/j2jthscnpkb5zu"
      ]
    },
    {
      "metadata": {
        "id": "2yMqk7MVoyfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running Example 2: Twitter Sentiment"
      ]
    },
    {
      "metadata": {
        "id": "BRzsAnmV6nYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Click into “Storage” from left-side menu, then you’ll see a page like the above. Click “Create bucket”"
      ]
    },
    {
      "metadata": {
        "id": "NX0a_rur6wDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img7.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "FqWGgJQZ65Y7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Go to Dataproc from the left side menu (you have to scroll down a bit. It’s under Big Data section) and click on “Clusters”. Click “Create clusters”"
      ]
    },
    {
      "metadata": {
        "id": "0eDfGezgohWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Get the Data"
      ]
    },
    {
      "metadata": {
        "id": "XXP1UILh3utG",
        "colab_type": "code",
        "outputId": "07378a1e-3faf-4a64-fde2-6123ee8a2d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "!unzip trainingandtestdata.zip   \n",
        "!rm trainingandtestdata.zip test*.csv\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2019-03-07 17:55:57--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  13.4MB/s    in 5.8s    \n",
            "\n",
            "2019-03-07 17:56:03 (13.4 MB/s) - ‘trainingandtestdata.zip’ saved [81363704/81363704]\n",
            "\n",
            "Archive:  trainingandtestdata.zip\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AqkWCSsO5HJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# set the names for each column\n",
        "cols = ['sentiment','id','date','query_string','user','text']\n",
        "def main():\n",
        "\t# read training data with ISO-8859-1 encoding and column names set above\n",
        "\tdf = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1',names=cols)\n",
        "\t# shuffle the data\n",
        "\tdf = df.sample(frac=1).reset_index(drop=True)\n",
        "\t# set the random seed and split train and test with 99 to 1 ratio\n",
        "\tnp.random.seed(777)\n",
        "\tmsk = np.random.rand(len(df)) < 0.99\n",
        "\ttrain = df[msk].reset_index(drop=True)\n",
        "\ttest = df[~msk].reset_index(drop=True)\n",
        "\t# save both train and test as CSV files\n",
        "\ttrain.to_csv('pyspark_sa_train_data.csv')\n",
        "\ttest.to_csv('pyspark_sa_test_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-JqDcJfqgu0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cd6x6bmQ6E9P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Upload pyspark_sa_train_data.csv and pyspark_sa_test_data.csv to your bucket. Verify the upload "
      ]
    },
    {
      "metadata": {
        "id": "bzuT6eoD6dG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img6.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "G9Qp9wib7qCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Submitting Spark Job. Finally, we are ready to run the training on Google Dataproc. Download the following code and submit it to your cluster as a pyspark job, pyspark_sa.py"
      ]
    },
    {
      "metadata": {
        "id": "pDMaScc95Li-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import sys\n",
        "import pyspark as ps\n",
        "import warnings\n",
        "import re\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql import types as t\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.ml.feature import Tokenizer, NGram, CountVectorizer, IDF, StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "# retrieve command line arguments and store them as variables\n",
        "inputdir = sys.argv[1]\n",
        "outputfile = sys.argv[2]\n",
        "modeldir = sys.argv[3]\n",
        "\n",
        "#define regex pattern for preprocessing\n",
        "pat1 = r'@[A-Za-z0-9_]+'\n",
        "pat2 = r'https?://[^ ]+'\n",
        "combined_pat = r'|'.join((pat1,pat2))\n",
        "www_pat = r'www.[^ ]+'\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
        "\n",
        "# preprocessing for\n",
        "# first_process: to remove Twitter handle and URL\n",
        "# second_process: to remove URL pattern starting with www.\n",
        "# third_process: to lower characters\n",
        "# fourth_process: to replace contracted negation with proper forms\n",
        "# result: remove numbers and special characters\n",
        "def pre_processing(column):\n",
        "    first_process = re.sub(combined_pat, '', column)\n",
        "    second_process = re.sub(www_pat, '', first_process)\n",
        "    third_process = second_process.lower()\n",
        "    fourth_process = neg_pattern.sub(lambda x: negations_dic[x.group()], third_process)\n",
        "    result = re.sub(r'[^A-Za-z ]','',fourth_process)\n",
        "    return result.strip()\n",
        "\n",
        "# build a pipeline following below order\n",
        "# tokenizer: split a tweet into individual words\n",
        "# ngrams: create n-gram representation of words. Here it's set to trigram\n",
        "# cv: turn n-gram representaion to a sparse representaion of the token counts. Below 5,460 is used as vocabulary size\n",
        "# idf: calculate inverse document frequency from the result of the previous step \n",
        "#      to diminishes the weight of terms that occur very frequently in the document set \n",
        "#      and increases the weight of terms that occur rarely.\n",
        "# assembler: transform the result of previous step to a single feature vector\n",
        "# label_stringIdx: encode target labels to a column of label indices. \n",
        "#                  The indices are ordered by label frequencies, so the most frequent label gets index 0.\n",
        "# lr: fit logistic regression with 'features' and 'label'\n",
        "def build_pipeline():\n",
        "\ttokenizer = [Tokenizer(inputCol='tweet',outputCol='words')]\n",
        "\tngrams = [NGram(n=i, inputCol='words', outputCol='{0}_grams'.format(i)) for i in range(1,4)]\n",
        "\tcv = [CountVectorizer(vocabSize=5460, inputCol='{0}_grams'.format(i), outputCol='{0}_tf'.format(i)) for i in range(1,4)]\n",
        "\tidf = [IDF(inputCol='{0}_tf'.format(i), outputCol='{0}_tfidf'.format(i), minDocFreq=5) for i in range(1,4)]\n",
        "\tassembler = [VectorAssembler(inputCols=['{0}_tfidf'.format(i) for i in range(1,4)], outputCol='features')]\n",
        "\tlabel_stringIdx = [StringIndexer(inputCol='sentiment', outputCol='label')]\n",
        "\tlr = [LogisticRegression(maxIter=100)]\n",
        "\tpipeline = Pipeline(stages=tokenizer+ngrams+cv+idf+assembler+label_stringIdx+lr)\n",
        "\treturn pipeline\n",
        "\n",
        "# below main function can be use for either first training or getting predictions with a loaded model\n",
        "# first retrieve data\n",
        "# apply pre-processing by making the above defined pre_processing function to a user defined function\n",
        "# either build the pipeline from the above build_pipeline function and train or use a loaded pipeline model\n",
        "# make predictions on the test set\n",
        "# output the pipeline model, Spark dataframe of the predictions, and the prediction accuracy on the test set\n",
        "def main(sqlc,input_dir,loaded_model=None):\n",
        "\tprint('retrieving data from {}'.format(input_dir))\n",
        "\tif not loaded_model:\n",
        "\t\ttrain_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_dir+'training_data.csv')\n",
        "\ttest_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(input_dir+'test_data.csv')\n",
        "\tprint('preprocessing data...')\n",
        "\treg_replaceUdf = f.udf(pre_processing, t.StringType())\n",
        "\tif not loaded_model:\n",
        "\t\ttrain_set = train_set.withColumn('tweet', reg_replaceUdf(f.col('text')))\n",
        "\ttest_set = test_set.withColumn('tweet', reg_replaceUdf(f.col('text')))\n",
        "\tif not loaded_model:\n",
        "\t\tpipeline = build_pipeline()\n",
        "\t\tprint('training...')\n",
        "\t\tmodel = pipeline.fit(train_set)\n",
        "\telse:\n",
        "\t\tmodel = loaded_model\n",
        "\tprint('making predictions on test data...')\n",
        "\tpredictions = model.transform(test_set)\n",
        "\taccuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_set.count())\n",
        "\treturn model, predictions, accuracy\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\t# create a SparkContext while checking if there is already SparkContext created\n",
        "\ttry:\n",
        "\t    sc = ps.SparkContext()\n",
        "\t    sc.setLogLevel(\"ERROR\")\n",
        "\t    sqlContext = ps.sql.SQLContext(sc)\n",
        "\t    print('Created a SparkContext')\n",
        "\texcept ValueError:\n",
        "\t    warnings.warn('SparkContext already exists in this scope')\n",
        "\t# build pipeline, fit the model and retrieve the outputs by running main() function\n",
        "\tpipelineFit, predictions, accuracy = main(sqlContext,inputdir)\n",
        "\tprint('predictions finished!')\n",
        "\tprint('accuracy on test data is {}'.format(accuracy))\n",
        "\t# select the original target label 'sentiment', 'text' and 'label' created by label_stringIdx in the pipeline\n",
        "\t# model predictions. Save it as a single CSV file to a destination specified by the second command line argument\n",
        "\tprint('saving predictions to {}'.format(outputfile))\n",
        "\tpredictions.select(predictions['sentiment'],predictions['text'],predictions['label'],predictions['prediction']).coalesce(1).write.mode(\"overwrite\").format(\"com.databricks.spark.csv\").option(\"header\", \"true\").csv(outputfile)\n",
        "\t# save the trained model to destination specified by the third command line argument\n",
        "\tprint('saving model to {}'.format(modeldir))\n",
        "\tpipelineFit.save(modeldir)\n",
        "\t# Load the saved model and make another predictions on the same test set\n",
        "\t# to check if the model was properly saved\n",
        "\tloadedModel = PipelineModel.load(modeldir)\n",
        "\t_, _, loaded_accuracy = main(sqlContext,inputdir,loadedModel)\n",
        "\tprint('accuracy with saved model on test data is {}'.format(loaded_accuracy))\n",
        "\tsc.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D6CLcZSR7-ok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img8.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "FK0W85gM8Djt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the above screenshot replace the blurred parts of the texts to your project ID, then click “submit” at the bottom. You can inspect the output of the machine by clicking into the job. The job is finished after 15 minutes"
      ]
    },
    {
      "metadata": {
        "id": "wqoMO55N8EiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img9.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "HOOWf-5U8fDh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Checking the Results. Go to your bucket, then go into pyspark_nlp folder. You will see that the results of the above Spark job have been saved into “result” directory (for the prediction data frame), and “model” directory (fitted pipeline model)."
      ]
    },
    {
      "metadata": {
        "id": "CdCmT0V48l_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img10.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "aDf3KIfg9Uw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Reference\n",
        "https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468"
      ]
    },
    {
      "metadata": {
        "id": "iE7RT7LipZQ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 1. "
      ]
    },
    {
      "metadata": {
        "id": "qcEh6qpmpS1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Download 10,000 Tweets about any topic you like (english language, must be a topic of divide i.e. having more than one view)\n",
        "\n",
        "2. Save them as csv and Upload Tweets to storage bucket\n",
        "3. Using PySpark run the sentiment analysis on those tweets using the model you trained earlier  \n",
        "https://gist.github.com/yanofsky/5436496"
      ]
    },
    {
      "metadata": {
        "id": "LppHOfZWgNVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Twitter Datasets\n",
        "\n",
        "https://data.world/datasets/twitter\n",
        "\n",
        "https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
        "\n",
        "https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment\n",
        "\n",
        "https://www.kaggle.com/shashank1558/preprocessed-twitter-tweets\n",
        "\n",
        "https://www.kaggle.com/kazanova/sentiment140"
      ]
    },
    {
      "metadata": {
        "id": "yKQJ_yh0pRDM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2. "
      ]
    },
    {
      "metadata": {
        "id": "08WpxyGkpeC9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Perform word count on your tweets\n",
        "2. Perform a word count on the different stances"
      ]
    }
  ]
}