{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3_PySpark.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msaadsadiq/BigDataCourse/blob/master/Assignment_3_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dEb4DtxF3y-W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Enabling APIs"
      ]
    },
    {
      "metadata": {
        "id": "7tV6wYMl7ype",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ECE 795 - Big Data\n",
        "## Assignment #3 - Twitter Sentiment Analysis using Distributed Computing on DataProc "
      ]
    },
    {
      "metadata": {
        "id": "P2dz95IU8QJb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Provide your credentials to the runtime"
      ]
    },
    {
      "metadata": {
        "id": "pZrs8Onl5Vns",
        "colab_type": "code",
        "outputId": "6d22269b-24dc-455d-b238-954af48c286b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Authenticate your student profile\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "48KU9aLS8Yd5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set the Project ID and Enable APIs"
      ]
    },
    {
      "metadata": {
        "id": "sUg7m-aj8Rr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_id = 'ece795'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxQ6TB_N8omQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.cloud import "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4v97QS235RN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### In GCP, there are many different services; Compute Engine, Cloud Storage, BigQuery, Cloud SQL, Cloud Dataproc to name a few. In order to use any of these services in your project, you first have to enable them."
      ]
    },
    {
      "metadata": {
        "id": "-XAex6YX3_Es",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*rYZZH8w9iScxIXG27qG-ww.png)"
      ]
    },
    {
      "metadata": {
        "id": "MJnvIcKk4Ldc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Put your mouse over “APIs & Services” on the left-side menu, then click into “Library”. For this project, we will enable three APIs: Cloud Dataproc, Compute Engine, and Cloud Storage."
      ]
    },
    {
      "metadata": {
        "id": "vz06iAlA4Grc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*qH5u_JSH2JLZW_SQTcetSQ.png)"
      ]
    },
    {
      "metadata": {
        "id": "fev7JQYYoje6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running Example 1: Word Count"
      ]
    },
    {
      "metadata": {
        "id": "N7VZj2MqqcmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### This word count example is similar to the one introduced earlier. It will use the Shakespeare dataset in BigQuery. The only difference is that instead of using Hadoop, it uses PySpark which is a Python library for Spark"
      ]
    },
    {
      "metadata": {
        "id": "V4E_-MsuqkvH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 1: create the output table in BigQuery. We need a table to store the output of our Map Reduce procedure.\n",
        "- Select your project or create a new one, remember to enable billing\n",
        "- Go to Big Query\n",
        "- Create a dataset, then a table using the following schema"
      ]
    },
    {
      "metadata": {
        "id": "YcsIufCZquoj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://)"
      ]
    },
    {
      "metadata": {
        "id": "I8eQM0BGoqmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2yMqk7MVoyfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running Example 2: Twitter Sentiment"
      ]
    },
    {
      "metadata": {
        "id": "0eDfGezgohWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Get the Data"
      ]
    },
    {
      "metadata": {
        "id": "XXP1UILh3utG",
        "colab_type": "code",
        "outputId": "cf290e87-28d4-42b7-9a52-e522b639bf0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "!unzip trainingandtestdata.zip   \n",
        "!rm trainingandtestdata.zip.1 test*.csv\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2019-03-04 22:47:26--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip.1’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  68.3MB/s    in 1.1s    \n",
            "\n",
            "2019-03-04 22:47:28 (68.3 MB/s) - ‘trainingandtestdata.zip.1’ saved [81363704/81363704]\n",
            "\n",
            "Archive:  trainingandtestdata.zip\n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AqkWCSsO5HJs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# set the names for each column\n",
        "cols = ['sentiment','id','date','query_string','user','text']\n",
        "def main():\n",
        "\t# read training data with ISO-8859-1 encoding and column names set above\n",
        "\tdf = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1',names=cols)\n",
        "\t# shuffle the data\n",
        "\tdf = df.sample(frac=1).reset_index(drop=True)\n",
        "\t# set the random seed and split train and test with 99 to 1 ratio\n",
        "\tnp.random.seed(777)\n",
        "\tmsk = np.random.rand(len(df)) < 0.99\n",
        "\ttrain = df[msk].reset_index(drop=True)\n",
        "\ttest = df[~msk].reset_index(drop=True)\n",
        "\t# save both train and test as CSV files\n",
        "\ttrain.to_csv('pyspark_sa_train_data.csv')\n",
        "\ttest.to_csv('pyspark_sa_test_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EpQI2jz5V3-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "cbGbaTXSV1HV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/python\n",
        "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "import json\n",
        "import pprint\n",
        "import subprocess\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "\n",
        "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
        "# by the InputFormat. This assumes the Cloud Storage connector for\n",
        "# Hadoop is configured.\n",
        "bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
        "project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
        "input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
        "\n",
        "conf = {\n",
        "    # Input Parameters.\n",
        "    'mapred.bq.project.id': project,\n",
        "    'mapred.bq.gcs.bucket': bucket,\n",
        "    'mapred.bq.temp.gcs.path': input_directory,\n",
        "    'mapred.bq.input.project.id': 'publicdata',\n",
        "    'mapred.bq.input.dataset.id': 'samples',\n",
        "    'mapred.bq.input.table.id': 'shakespeare',\n",
        "}\n",
        "\n",
        "# Output Parameters.\n",
        "output_dataset = 'wordcount_dataset'\n",
        "output_table = 'wordcount_output'\n",
        "\n",
        "# Load data in from BigQuery.\n",
        "table_data = sc.newAPIHadoopRDD(\n",
        "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
        "    'org.apache.hadoop.io.LongWritable',\n",
        "    'com.google.gson.JsonObject',\n",
        "    conf=conf)\n",
        "\n",
        "# Perform word count.\n",
        "word_counts = (\n",
        "    table_data\n",
        "    .map(lambda record: json.loads(record[1]))\n",
        "    .map(lambda x: (x['word'].lower(), int(x['word_count'])))\n",
        "    .reduceByKey(lambda x, y: x + y))\n",
        "\n",
        "# Display 10 results.\n",
        "pprint.pprint(word_counts.take(10))\n",
        "\n",
        "# Stage data formatted as newline-delimited JSON in Cloud Storage.\n",
        "output_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output'.format(bucket)\n",
        "output_files = output_directory + '/part-*'\n",
        "\n",
        "sql_context = SQLContext(sc)\n",
        "(word_counts\n",
        " .toDF(['word', 'word_count'])\n",
        " .write.format('json').save(output_directory))\n",
        "\n",
        "# Shell out to bq CLI to perform BigQuery import.\n",
        "subprocess.check_call(\n",
        "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
        "    '--replace '\n",
        "    '--autodetect '\n",
        "    '{dataset}.{table} {files}'.format(\n",
        "        dataset=output_dataset, table=output_table, files=output_files\n",
        "    ).split())\n",
        "\n",
        "# Manually clean up the staging_directories, otherwise BigQuery\n",
        "# files will remain indefinitely.\n",
        "input_path = sc._jvm.org.apache.hadoop.fs.Path(input_directory)\n",
        "input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(input_path, True)\n",
        "output_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\n",
        "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
        "    output_path, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yKQJ_yh0pRDM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 1. "
      ]
    },
    {
      "metadata": {
        "id": "qcEh6qpmpS1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Crawl 10,000 Tweets about any topic you like (english language, must be a topic of divide i.e. having more than one view)\n",
        "2. Upload them to storage bucket\n",
        "3. Using PySpark run the sentiment analysis on those tweets on the model you trained earlier  "
      ]
    },
    {
      "metadata": {
        "id": "iE7RT7LipZQ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2. "
      ]
    },
    {
      "metadata": {
        "id": "08WpxyGkpeC9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Perform word count on your tweets\n",
        "2. Performa word count on the different stances"
      ]
    }
  ]
}