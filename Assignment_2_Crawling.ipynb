{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2_Crawling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LUhZiGtTxGKQ",
        "-wW8Xr9wxffX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Sjx11HUOtUe4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ECE 795 - Big Data - Assignment #2\n",
        "#  Crawling Data from the Internet to BigQuery\n"
      ]
    },
    {
      "metadata": {
        "id": "DswGSqjAuPC3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Before you begin\n",
        "**BE AWARE that you can only do a maximum of 96 DML (update, insert, delete) operations on a table per day. Be careful when you do such task in the assignment and start to work on the assignment several days before the deadline**\n",
        "\n",
        "[New York Social Diary](http://www.newyorksocialdiary.com/) is a website providing news are reports about the social life of the elite New Yorkers. Take a look at one of [the recent post](http://www.newyorksocialdiary.com/social-diary/2019/in-the-mood). Among these news and articles, you will be able to scatch the social network of the New York's social elites.\n",
        "\n",
        "In this project, we will utlize the website and crawl data from it to obtain some latent information in the news and articles. All the photos are carefully captioned and labelled with those who appear in the photos. This give us a perfect chance to analyze the connections between these people. We can assume that people, who appear in the same picture together, have a connection. \n",
        "\n",
        "For this assignment, we will assemble the social graph from photo captions, store the data in BigQuery, and perform some basic analysis on the data."
      ]
    },
    {
      "metadata": {
        "id": "QaxhiXy2Au2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Crawling Data"
      ]
    },
    {
      "metadata": {
        "id": "iyGWIcslA0lJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get list of \"party pictures\" from the website"
      ]
    },
    {
      "metadata": {
        "id": "TDIToDQvBH8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take a look at [the archive page of party pictures](http://www.newyorksocialdiary.com/party-pictures).\n",
        "You will find a list of links, each of which points at a post of party pictures. On each page, 50 links are presented.\n",
        "\n",
        "In the following, we will get the url for each party page, along with its date.\n",
        "\n",
        "Here are some packages that you may find useful.  You are welcome to use others, if you prefer."
      ]
    },
    {
      "metadata": {
        "id": "NxcxVY-ZCxXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R41_sBwfeKgR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Download HTML of webpages\n",
        "If you open a website with any browser, your browser's developer tools (usually `Cmd`-`Option`-`I` on Mac, `Ctrl`-`Shift`-`I` on others) offer helpful tools to explore the structure of the HTML page. You can also right click the page and select \"Inspect\"  to achieve the same purpose.\n",
        "\n",
        "In the inspection window, you will be able to see the structures of the webpage. They are the codes in [HTML language](https://en.wikipedia.org/wiki/HTML) that tell your browser what to display .\n",
        "In this project, we will extract useful information from HTML.\n",
        "We recommend using Python [Requests](http://docs.python-requests.org/en/master/) to download the HTML pages, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to process the HTML. \n",
        "\n",
        "Let's start by getting the [first page of \"party pictures\"](http://www.newyorksocialdiary.com/party-pictures).*"
      ]
    },
    {
      "metadata": {
        "id": "hUswtB-Cfuc3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "page = requests.get(\"http://www.newyorksocialdiary.com/party-pictures\") # Use requests.get to download the page."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S586p9icgEOP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Parse HTML with BeautifulSoup\n",
        "BeautifulSoup provides a parser to understand the structure of HTML. After investigate the structure of the webpage and find a pattern, you can then use BeautifulSoup's [select](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors) or [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) methods to get those elements."
      ]
    },
    {
      "metadata": {
        "id": "Vrf0JoP8hqQe",
        "colab_type": "code",
        "outputId": "72f111ac-7335-4adf-adb9-36e1eca822c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.text, \"lxml\")\n",
        "links = soup.find_all('div', attrs={'class':'views-row'})\n",
        "print(links[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<div class=\"views-row views-row-1 views-row-odd views-row-first\">\n",
            "<span class=\"views-field views-field-title\"> <span class=\"field-content\"><a href=\"/party-pictures/2019/touring-and-traveling-back-in-time\">Touring and traveling back in time</a></span> </span>\n",
            "<span class=\"views-field views-field-created\"> <span class=\"field-content\">Monday, February 11, 2019</span> </span> </div>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5e1MVAhhiD_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We should be able to find 50 links from the webpage."
      ]
    },
    {
      "metadata": {
        "id": "xIjpeOIviKdu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert len(links) == 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "blQyWbwwib--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at that first link.  Figure out how to extract the URL of the link, as well as the date.  You probably want to use `datetime.strptime`.  See the [format codes for dates](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior) for reference."
      ]
    },
    {
      "metadata": {
        "id": "3zu1eQENigOt",
        "colab_type": "code",
        "outputId": "c0ba09ee-da5a-4fb4-d564-4d55bd4139a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Check that the title and date match what you see visually.\n",
        "first_link = links[0]\n",
        "first_url = first_link.find('a').attrs['href']\n",
        "first_date = datetime.strptime(\"\".join(first_link.find_all('span',{'class':'field-content'})[1].get_text()), '%A, %B %d, %Y')\n",
        "print(first_url, first_date)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/party-pictures/2019/touring-and-traveling-back-in-time 2019-02-11 00:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ooRlpnuOivXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For purposes of code reuse, let's put that logic into a function.  It should take the link element and return the URL and date parsed from it."
      ]
    },
    {
      "metadata": {
        "id": "A8TBAaG0iz12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_link_date(links, el):\n",
        "    url_temp = links[el].find('a').attrs['href']\n",
        "    date = datetime.strptime(\"\".join(links[el].find_all('span',{'class' : 'field-content'})[1].contents), '%A, %B %d, %Y')\n",
        "    return [url_temp, date]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixBDyyWii4GH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, when we want to obtain the 50-th element of the page, we can use the following link. Be aware that the index starts from 0."
      ]
    },
    {
      "metadata": {
        "id": "HKd1Ehp1i11n",
        "colab_type": "code",
        "outputId": "0ddf5ed5-70dc-469a-e6cf-cda45f9e10ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "get_link_date(links, 49)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/party-pictures/2018/in-the-spotlight', datetime.datetime(2018, 8, 2, 0, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "7eZ5J5Vfk_XL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You may want to check that it works as you expected.\n",
        "\n",
        "Once that's working, let's write another function to parse all of the links on a page.  Thinking ahead, we can make it take a Requests [Response](http://docs.python-requests.org/en/master/api/#requests.Response) object and do the BeautifulSoup parsing within it."
      ]
    },
    {
      "metadata": {
        "id": "f7fm5cC1lFxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_links(response):\n",
        "    soup = BeautifulSoup(response.text, \"lxml\")\n",
        "    links = soup.find_all('div', class_='views-row')\n",
        "    page_links = []\n",
        "    for i in range(len(links)):\n",
        "        page_links.append(get_link_date(links, i))\n",
        "    return page_links\n",
        "#   return ... # A list of URL, date pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wXuf09w_lPYr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we run this on the previous response, we should get 50 pairs."
      ]
    },
    {
      "metadata": {
        "id": "NFSuiWv0lQEF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert len(get_links(page)) == 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCQEGcy4l02k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we should be ready to get all of the party URLs.  Click through a few of the index pages to determine how the URL changes.  Figure out a strategy to visit all of them.\n",
        "\n",
        "HTTP requests are generally IO-bound.  This means that most of the time is spent waiting for the remote server to respond.  If you use `requests` directly, you can only wait on one response at a time.  [requests-futures](https://github.com/ross/requests-futures) lets you wait for multiple requests at a time.  You may wish to use this to speed up the downloading process."
      ]
    },
    {
      "metadata": {
        "id": "iegIbchRl1_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://www.newyorksocialdiary.com/party-pictures\"\n",
        "response = requests.get(url, params={\"page\": \"\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNtiKwNBmjxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_page_args(i):\n",
        "    return {\"url\": url,\n",
        "            \"params\": {\"page\": i}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UxdFY2KhmlJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from requests_futures.sessions import FuturesSession\n",
        "# You can use link_list.extend(others) to add the elements of others\n",
        "# to link_list.\n",
        "\n",
        "link_list = []\n",
        "\n",
        "session = FuturesSession(max_workers=5)\n",
        "futures = [session.get(**get_page_args(i)) for i in range(32)]\n",
        "\n",
        "\n",
        "#link_list is capturing links for each page, creating a list of lists\n",
        "# link_list = [get_links(future.result()) for future in futures]\n",
        "\n",
        "#this captures each list one at a time and adds it to a single list, link_list.\n",
        "for future in futures:\n",
        "    link_list.extend(get_links(future.result()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZK3PVpZnAdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It the request_futures library is not avaliable in Colab, you can use the following command to install it"
      ]
    },
    {
      "metadata": {
        "id": "F1TpRKGjnJfl",
        "colab_type": "code",
        "outputId": "24383e95-7e38-4f06-c4b7-35330fed83b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install requests_futures"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests_futures in /usr/local/lib/python3.6/dist-packages (0.9.9)\n",
            "Requirement already satisfied: requests>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from requests_futures) (2.18.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.0->requests_futures) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.0->requests_futures) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.0->requests_futures) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.2.0->requests_futures) (1.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BBO3HwGwsZYH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Store Crawled Data into BigQuery Table\n",
        "\n",
        "In order to store the data into BigQuery, we need to first create a table with appropriate schema in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "LUhZiGtTxGKQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Provide your credentials to the runtime"
      ]
    },
    {
      "metadata": {
        "id": "z_8JntGHxOPI",
        "colab_type": "code",
        "outputId": "c7dab2b1-97c7-4319-9594-51a4c0b00931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Authenticate your student profile\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-wW8Xr9wxffX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a new dataset in BigQuery to store your own data"
      ]
    },
    {
      "metadata": {
        "id": "uN6gI71r27nV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set the project_id as you did in assignment 1 and specify your desired dataset_id (letters, numbers and underscores allowed).\n",
        "\n",
        "After excuting the code, you should be able to see the created dataset in [BigQuery Web UI](https://console.cloud.google.com/bigquery) under your project_id."
      ]
    },
    {
      "metadata": {
        "id": "PzKTQUfd2Rp-",
        "colab_type": "code",
        "outputId": "004f711f-424f-43bc-ca54-b57c4de0a44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import Conflict\n",
        "\n",
        "project_id = 'bigdataproject-231517'  # Set your project_id here\n",
        "client = bigquery.Client(project=project_id)\n",
        "dataset_id = 'assignment2_dataset'  # Specify you dataset_id here\n",
        "\n",
        "# Create a DatasetReference using a chosen dataset ID.\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "\n",
        "# Construct a full Dataset object to send to the API.\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "# Specify the geographic location where the dataset should reside.\n",
        "dataset.location = \"US\"\n",
        "\n",
        "# Send the dataset to the API for creation.\n",
        "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "# exists within the project.\n",
        "try:\n",
        "   dataset = client.create_dataset(dataset)  # API request\n",
        "except Conflict:\n",
        "   print('Dataset exists')\n",
        "print('Dataset Created')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset exists\n",
            "Dataset Created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x27tOa7-6CC8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a new table for the following examples"
      ]
    },
    {
      "metadata": {
        "id": "Sct76LM07jjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before creating a table, you need to properly design the table scheme. In this example, we will create a table with three attributes: an integer as partyID, a string as the link, and an array of integer as the date."
      ]
    },
    {
      "metadata": {
        "id": "MgIaV1m-6TZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "schema = [\n",
        "    bigquery.SchemaField('partyID', 'INTEGER', mode='REQUIRED'),\n",
        "    bigquery.SchemaField('partyLink', 'STRING', mode='REQUIRED'),\n",
        "    bigquery.SchemaField('partyDate', 'DATE', mode='REQUIRED')\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j5pmiXcE2szx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, you can create the table with GCP API."
      ]
    },
    {
      "metadata": {
        "id": "aL5JmM3T2xVp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "table_ref = dataset_ref.table('party')\n",
        "table = bigquery.Table(table_ref, schema=schema)\n",
        "table = client.create_table(table)  # API request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uxxkDru4ZTg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can verify whether the table is created successfully by cheking the BigQuery Console and running the following assert"
      ]
    },
    {
      "metadata": {
        "id": "hGJKj-mu4jTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert table.table_id == 'party'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxGt0NaC4n1X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Insert crawled data into the table\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XoqCKzFB71n-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "BigQuery only accept the format of Date as YYYY-MM-DD, so we need to convert the date into this format before inserting them into the table. We recommend to use [datetime.strftime](https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior) function for this purpose."
      ]
    },
    {
      "metadata": {
        "id": "9ZPY9Nl06FN3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "link_list = [(i, link_list[i][0], link_list[i][1].strftime(\"%Y-%m-%d\")) for i in range(len(link_list))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KB2YdG7d9i-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we can stream the data into the table as mentioned in the [GCP documentation](https://cloud.google.com/bigquery/streaming-data-into-bigquery). You can also import the data from local files and Cloud Storage. It might take a while before all the data goes into the streaming buffer of the table.\n",
        "\n",
        "There are [quota policies](https://cloud.google.com/bigquery/quotas#streaming_inserts) of streaming inserts into BigQuery."
      ]
    },
    {
      "metadata": {
        "id": "7-SEGidi91Fc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "errors = client.insert_rows(table, link_list)  # API request\n",
        "assert errors == []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEZLyHVoCMVA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After the data appear in the streaming buffer, you may execute the queries as you did in assignment 1. The following example query the total number of entries in the table."
      ]
    },
    {
      "metadata": {
        "id": "ao9f33S-A-B-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "row_count = client.query('''\n",
        "  SELECT \n",
        "    COUNT(*) as total\n",
        "  FROM `assignment2_dataset.party`''').to_dataframe().total[0]\n",
        "assert row_count == len(link_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAiAp6WuDAcN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assignments\n",
        "**In this assignment, you need to investigate the structure of the captions in the posts of \"party pictures\", design a method to extract necessary information according to the structure, insert data into the table, and design query to analyze them**"
      ]
    },
    {
      "metadata": {
        "id": "GxTyKP2IE8IW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 1: Design a query to get all the links in table \"party\" with the date before December 1, 2014 (including December 1, 2014)."
      ]
    },
    {
      "metadata": {
        "id": "wYDGTlLqF0A9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZEzVJ3RF2rN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 2: Investigate the structure of the obtained links from Question1 and design a method to crawl the image captions from the webpages."
      ]
    },
    {
      "metadata": {
        "id": "7JePty_KGvrb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6peITbnGxID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 3: Create a new table in BigQuery as the following schema, and then store the obtained captions into the table appropriately."
      ]
    },
    {
      "metadata": {
        "id": "lfgpuxenHMrK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "schema2 = [\n",
        "    bigquery.SchemaField('partyID', 'INTEGER', mode='REQUIRED'),\n",
        "    bigquery.SchemaField('imageID', 'INTEGER', mode='REQUIRED'),\n",
        "    bigquery.SchemaField('captionText', 'STRING', mode='REQUIRED')\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lY2yaG6VIdA6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ux4W_PQdIfoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 4: Design a method to extract names in the caption and store the obtained names into another new table."
      ]
    },
    {
      "metadata": {
        "id": "eU40VB-AJPq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are various methods that can be utilized to extract names from the captions. The details are up to you.\n",
        "\n",
        "Please find the following some hints to implement the method. You do not have to follow all of the hint to achieve a good result. It's okay that if you do not follow any of the hint. This question will be evaluated by the quality of your results.\n",
        "  1. Some of the captions do not include names, instead,  they can be long narrative texts describe the photos or the events. You can implement some heuristic rules to filter out these captions. For example,\n",
        "    - Names should be lists of nouns. There are various way to delilver this, for example, a straightforward but slow way is using [`nltk` library in Python to tag part of speech](http://www.nltk.org/book/ch05.html). Alternatively, spaCy's [entity recognition](https://spacy.io/docs/usage/entity-recognition) can detect whether the word is name or not.\n",
        "    - Look for common patterns in the captions with names (e.g. looking for the phrase `a friend`).\n",
        "    - It is often not lists of names if the caption is very long. (You may use 250 characters as cutoff)\n",
        "  2. The captions are in various forms and you will need to separate them accordingly. You may want to use `re.split`, which is more sophisticated but more powerful than `string.split`.\n",
        "  3. \"ra Lebenthal\" might be found as a name but it is not. You might need to investigate what happens here.\n",
        "  4. The same person might be referenced with the title, such as \"Mayor Michael Bloomberg\", or not \"Michael Bloomberg\". The title might change over the time as well. The titles might need to be filtered out so all these names can ultimately refer to the same person: \"Michael Bloomberg.\"\n",
        "  5. Sometimes, the couple might be written as eg. \"John and Mary Smith\" referring to \"John Smith\" and \"Mary Smith\". You might need to handle it.\n",
        "  6. Check you output frequently and figure out the errors that come up. Then, you can fix the errors iteratively until you have a list that looks reasonable.\n",
        "  7. You will probably find that the webpages have a slightly different HTML structures, as well as new captions fail your name parser. But don't worry if the parser isn't perfect -- just try to get the easy cases.\n",
        "\n",
        "Once you feel that your algorithm is working well on these captions, parse all of the captions and extract all the names mentioned. Sort them alphabetically, by first name, and insert them into a new table with the given schema.\n",
        "\n",
        "In this task, we neglect the fact that there can be multiple persons with the same name."
      ]
    },
    {
      "metadata": {
        "id": "7LNDjuqSJG_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "schema3 = [\n",
        "    bigquery.SchemaField('imageID', 'INTEGER', mode='REQUIRED'),,\n",
        "    bigquery.SchemaField('personName', 'STRING', mode='REQUIRED')\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gq_ufBGoLFSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0UvSLzwdLIVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 5: Who is the most popular?\n",
        "The easiest way to answer this question is to look at how many connections everyone has. If a person has been in the same picture with 5 identical people, he has 5 connections.\n",
        "Design a query and find the top 100 people and their number of connections."
      ]
    },
    {
      "metadata": {
        "id": "a93pcVCfMXuo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
